# Parallel-Systems-MPI

# Άσκηση 3.1 — Παράλληλος Πολλαπλασιασμός Πολυωνύμων

## Εισαγωγή

Η παρούσα εργασία πραγματεύεται την υλοποίηση ενός παράλληλου αλγορίθμου πολλαπλασιασμού πολυωνύμων χρησιμοποιώντας το πρότυπο επικοινωνίας μηνυμάτων **MPI (Message Passing Interface)**. Στόχος είναι η αποδοτική εκμετάλλευση πολλαπλών διεργασιών για την επιτάχυνση του υπολογισμού, ακολουθώντας τις προδιαγραφές της εκφώνησης και τις αρχές του διανεμημένου υπολογισμού.

## Θεωρητικό Υπόβαθρο

Ο πολλαπλασιασμός δύο πολυωνύμων βαθμού *n* με κλασική συνέλιξη έχει υπολογιστική πολυπλοκότητα **O(n²)**. Παρότι υπάρχουν ταχύτερες μέθοδοι όπως ο **FFT**, η εκφώνηση απαιτεί ρητά τη χρήση της κλασικής προσέγγισης. Η μέθοδος αυτή είναι ιδιαίτερα κατάλληλη για παράλληλη εκτέλεση λόγω της ανεξαρτησίας των επιμέρους υπολογισμών.

## Μοντέλο Παραλληλισμού

Η υλοποίηση ακολουθεί το μοντέλο **SPMD (Single Program Multiple Data)**, όπου όλες οι διεργασίες εκτελούν το ίδιο πρόγραμμα σε διαφορετικά υποσύνολα δεδομένων. Η κατανομή της εργασίας γίνεται με βάση τους συντελεστές του πρώτου πολυωνύμου.

## Σχεδιαστικές Επιλογές

### Κατανομή Δεδομένων

Το πολυώνυμο **A** κατανέμεται στις διεργασίες με *block distribution* χρησιμοποιώντας `MPI_Scatterv`, επιτρέποντας ανισομερή κατανομή όταν το `n+1` δεν διαιρείται ακριβώς με τον αριθμό διεργασιών. Αυτό περιορίζει το **load imbalance** και βελτιώνει τη συνολική απόδοση.

Το πολυώνυμο **B** μεταδίδεται ακέραιο σε όλες τις διεργασίες μέσω `MPI_Bcast`, καθώς κάθε διεργασία χρειάζεται πλήρη πρόσβαση στους συντελεστές του για τον υπολογισμό της συνέλιξης.

### Τοπικός Υπολογισμός

Κάθε διεργασία υπολογίζει ένα τμήμα του τελικού αποτελέσματος χρησιμοποιώντας την κλασική μέθοδο συνέλιξης. Ο υπολογισμός είναι πλήρως ανεξάρτητος μεταξύ διεργασιών, αποφεύγοντας την ανάγκη συγχρονισμού κατά τη φάση υπολογισμού.

### Συλλογή Αποτελεσμάτων

Τα μερικά αποτελέσματα αποθηκεύονται σε τοπικούς πίνακες και συγχωνεύονται στη διεργασία 0 μέσω `MPI_Reduce` με πράξη άθροισης. Η προσέγγιση αυτή ακολουθεί το πρότυπο **compute-locally, reduce-globally**.

## Χρονομέτρηση και Απόδοση

Η εκτέλεση χωρίζεται σε τρία βασικά στάδια: **επικοινωνία**, **υπολογισμό** και **συλλογή αποτελεσμάτων**. Η χρήση του `MPI_Wtime` επιτρέπει ακριβή μέτρηση των επιμέρους χρόνων και του συνολικού χρόνου εκτέλεσης.

## Πειραματική Αξιολόγηση & Scalability

Η απόδοση του προγράμματος αξιολογήθηκε για διάφορους βαθμούς πολυωνύμων (*n = 10⁵, 10⁶*) και διαφορετικό αριθμό διεργασιών MPI.

### Speedup

Ορίζεται ως:  
`Speedup(p) = T₁ / Tₚ`

Παρατηρείται σχεδόν γραμμική επιτάχυνση για μικρό και μεσαίο αριθμό διεργασιών. Για μεγαλύτερο αριθμό διεργασιών, το κόστος επικοινωνίας αρχίζει να επηρεάζει τη συνολική απόδοση.

### Efficiency

`Efficiency(p) = Speedup(p) / p`

Η αποδοτικότητα μειώνεται όσο αυξάνεται το *p*, κυρίως λόγω του κόστους επικοινωνίας (`Scatterv`, `Bcast`, `Reduce`).

### Ανάλυση Bottleneck

Το βασικό bottleneck εντοπίζεται:
- στο `MPI_Bcast` για μεγάλες τιμές *n*,
- και στο `MPI_Reduce` κατά τη συλλογή του αποτελέσματος.

Παρά ταύτα, το συνολικό όφελος παραλληλισμού παραμένει σημαντικό για μεγάλα προβλήματα.

### Επίδραση Overhead Επικοινωνίας & Σύγκριση με Shared Memory

#### Overhead Επικοινωνίας

Στο μοντέλο shared memory (Εργασία 1), η επικοινωνία μεταξύ νημάτων είναι εξαιρετικά γρήγορη, καθώς μοιράζονται την ίδια μνήμη.  
Στο MPI, τα δεδομένα μεταφέρονται ρητά μέσω μηχανισμών επικοινωνίας, γεγονός που αυξάνει το κόστος.

Για μικρά προβλήματα (*N ≈ 10.000*), το MPI συχνά εμφανίζει χαμηλότερο speedup, καθώς ο χρόνος επικοινωνίας υπερκαλύπτει το κέρδος από τον παραλληλισμό.

#### Scalability

Για μεγάλα μεγέθη (*N ≥ 50.000*), ο υπολογισμός **O(N²)** κυριαρχεί έναντι του κόστους επικοινωνίας **O(N)**.  
Σε αυτή την περιοχή, το MPI παρουσιάζει καλύτερη κλιμάκωση σε σχέση με shared-memory υλοποιήσεις, οι οποίες συχνά κορεσµούνται λόγω περιορισμένου memory bandwidth.

#### Memory Bottleneck

Στο shared memory, όλα τα νήματα ανταγωνίζονται για την ίδια RAM, δημιουργώντας **memory bandwidth bottleneck**.  
Στο MPI (ιδίως σε πραγματικό cluster), κάθε κόμβος διαθέτει ανεξάρτητη μνήμη, μειώνοντας τη συμφόρηση και επιτρέποντας καλύτερη κλιμάκωση.

---

# Άσκηση 3.2 — Παράλληλος Πολλαπλασιασμός Αραιού Πίνακα με Διάνυσμα (SpMV)

## Περιγραφή Προβλήματος

Η δεύτερη άσκηση αφορά την αποδοτική υλοποίηση του πολλαπλασιασμού **αραιού πίνακα με διάνυσμα** (SpMV) με χρήση της αναπαράστασης **Compressed Sparse Row (CSR)** και του MPI.

## Θεωρητικό Υπόβαθρο

Η πράξη SpMV έχει πολυπλοκότητα **O(nnz)** και υπερτερεί της πυκνής μορφής **O(N²)** όταν ο πίνακας είναι αραιός.

## Μοντέλο Παραλληλισμού

Οι γραμμές του πίνακα κατανέμονται ισομερώς στις διεργασίες και το διάνυσμα αναπαράγεται σε όλες τις διεργασίες.

## Σχεδιαστικές Επιλογές Υλοποίησης

### Κατασκευή & Διανομή CSR

Η διεργασία 0 κατασκευάζει τον CSR πίνακα και διαμοιράζει μόνο τα απαραίτητα δεδομένα με `MPI_Scatterv` και `MPI_Bcast`.

### Παράλληλος Υπολογισμός SpMV

Κάθε διεργασία εκτελεί τοπικά τον υπολογισμό και το αποτέλεσμα συγχρονίζεται με `MPI_Allgather`.

### Σύγκριση με Πυκνή Αναπαράσταση

Η CSR προσέγγιση υπερτερεί σημαντικά για ποσοστά αραιότητας ≥ 70–80%.

## Συμπεράσματα

Η εργασία καταδεικνύει τη σημασία της σωστής αναπαράστασης δεδομένων και του μοντέλου παραλληλισμού για την επίτευξη υψηλής απόδοσης σε υπολογιστικά εντατικές εφαρμογές.
